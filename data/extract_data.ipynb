{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "def segment_sql_commands(sql_file):\n",
    "    commands = []\n",
    "    current_command = \"\"\n",
    "    in_comment_block = False\n",
    "\n",
    "    with open(sql_file, 'r') as file:\n",
    "        try:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "\n",
    "                if not line or line.startswith('--'):\n",
    "                    continue\n",
    "\n",
    "                if line.startswith('/*') and line.endswith('*/;'):\n",
    "                    continue\n",
    "                elif line.startswith('/*'):\n",
    "                    in_comment_block = True\n",
    "                    continue\n",
    "                elif line.endswith('*/;'):\n",
    "                    in_comment_block = False\n",
    "                    continue\n",
    "                \n",
    "                if in_comment_block:\n",
    "                    continue\n",
    "\n",
    "                current_command += line + ' '\n",
    "\n",
    "                if line.endswith(';'):\n",
    "                    current_command = current_command.replace(' unsigned','')\n",
    "                    current_command = current_command.replace(' AUTO_INCREMENT','')\n",
    "                    commands.append(current_command.strip())\n",
    "                    current_command = \"\"\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "\n",
    "    return commands\n",
    "\n",
    "# sql_file = \"libgen_compact.sql\"\n",
    "sql_file = \"fiction.sql\"\n",
    "segmented_commands = segment_sql_commands(sql_file)\n",
    "comms = segmented_commands[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,1000):\n",
    "    with open(\"statements_fiction/statement\"+ str(i) + \".txt\",'w') as file:\n",
    "        file.write(comms[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step was to reduce the statement to a more manageable chunk. But I think this won't be necessary anymore so move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows(sql_insert):\n",
    "    start_idx = sql_insert.find('(')\n",
    "    end_idx = sql_insert.rfind(')')\n",
    "    if start_idx == -1 or end_idx == -1:\n",
    "        raise ValueError(\"Invalid SQL insert statement\")\n",
    "\n",
    "    rows = sql_insert[start_idx + 1:end_idx]\n",
    "    return rows.split('),(')\n",
    "\n",
    "def create_new_insert(original_insert, n):\n",
    "    rows = extract_rows(original_insert)\n",
    "    new_rows = rows[:n]\n",
    "    new_insert = original_insert.split('(')[0] + '(' + '),('.join(new_rows) + ');'\n",
    "    return new_insert\n",
    "\n",
    "def main(input_file, output_file, n):\n",
    "    with open(input_file, 'r') as f:\n",
    "        sql_insert = f.read()\n",
    "\n",
    "    new_insert = create_new_insert(sql_insert, n)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(new_insert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"statement.txt\", \"reduced_statement.txt\", 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now skip the next code block too. We'll be using \"sql_to_csv_NEW\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_csv(statement_file,out_file):\n",
    "    with open(statement_file,'r') as file:\n",
    "        insert = file.readline()\n",
    "        m1 = re.findall('`(.*?)`', insert)\n",
    "        columns = m1[1:]\n",
    "        print(columns,len(columns))\n",
    "        s=r'(.*?),'*46\n",
    "        # print(r'\\('+s+r'(.*?)\\)')\n",
    "        rows = re.findall(r'\\('+s+r'(.*?)\\)', insert)\n",
    "\n",
    "        with open(out_file,'w') as file2:\n",
    "            pass\n",
    "        with open(out_file,'a') as file2:\n",
    "            for row in rows:\n",
    "                file2.write(str(row[0])+ \" \" + row[1] + \" \" + row[5] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Title', 'VolumeInfo', 'Series', 'Periodical', 'Author', 'Year', 'Edition', 'Publisher', 'City', 'Pages', 'PagesInFile', 'Language', 'Topic', 'Library', 'Issue', 'Identifier', 'ISSN', 'ASIN', 'UDC', 'LBC', 'DDC', 'LCC', 'Doi', 'Googlebookid', 'OpenLibraryID', 'Commentary', 'DPI', 'Color', 'Cleaned', 'Orientation', 'Paginated', 'Scanned', 'Bookmarked', 'Searchable', 'Filesize', 'Extension', 'MD5', 'Generic', 'Visible', 'Locator', 'Local', 'TimeAdded', 'TimeLastModified', 'Coverurl', 'Tags', 'IdentifierWODash'] 47\n"
     ]
    }
   ],
   "source": [
    "sql_to_csv(\"sql.txt\",\"titles_authors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(3000):\n",
    "#     search = \"(\"+str(start+k)+\",\"\n",
    "#     search2 = \"(\"+str(start+k+1)+\",\"\n",
    "#     search3 = \"(\"+str(start+k+4)+\",\"\n",
    "#     index = insert.find(search)\n",
    "#     index2 = insert.find(search2)\n",
    "#     index3 = insert.find(search3)\n",
    "#     # print(index)\n",
    "#     if index==-1 and index2==-1 and index3==-1:\n",
    "#         last_index = start+k-1\n",
    "#         break\n",
    "#     elif index==-1 and index2!=-1:\n",
    "#         continue\n",
    "#     elif index==-1 and index2==-1 and index3!=-1:\n",
    "#         k = k+3\n",
    "#         continue\n",
    "\n",
    "# for k in range(3000):\n",
    "#     for i in range(11):\n",
    "#         search = \"(\" + str(start + k + i) + \",\"\n",
    "#         index = insert.find(search)\n",
    "#         if index != -1:\n",
    "#             break\n",
    "    \n",
    "#     if index==-1:\n",
    "#         last_index=start+k-1\n",
    "#         break\n",
    "#     else:\n",
    "#         k=k+i\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important fields for me: ID (0), Title (1), Series 3, Periodical 4, Author 5, Publisher 8, City 9, Language 12, Topic 13, Library 14, Commentary 26, Coverurl 44, Tags 45\n",
    "\n",
    "Indian name classifier: https://github.com/vsant/indian-name-classifier\n",
    "\n",
    "So we just classify the names? Or is there something more interesting we can do. If there's some sort of textual information, like a summary, we can think of a sort of language model to understand if the context is indian or not. Depending on the data we have we can think of training it or not. Training is always better if the data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_csv_NEW(statement_file,out_file,start):\n",
    "    with open(out_file,'w') as file:\n",
    "        pass\n",
    "    with open(statement_file,'r') as file:\n",
    "        with open(out_file,'a') as file2:\n",
    "            insert = file.readline()\n",
    "\n",
    "            # print(\"Searching from \",start,\" to +3000\")\n",
    "\n",
    "            assert insert.find(\"(\"+str(start)+\",\")!=-1\n",
    "\n",
    "            found_last = False\n",
    "            last_index = None\n",
    "\n",
    "            for i in range(len(insert)-1,len(insert)-3000,-1):\n",
    "                # print(i)\n",
    "                if(insert[i]=='('):\n",
    "                    string = insert[i:i+11]\n",
    "                    # print(string)\n",
    "                    comma_index = string.find(\",\")\n",
    "                    # print(comma_index,string[:comma_index])\n",
    "                    if comma_index==-1:\n",
    "                        continue\n",
    "                    elif not is_integer(string[1:comma_index]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        last_index = int(string[1:comma_index])\n",
    "                        break\n",
    "\n",
    "            end = last_index\n",
    "            \n",
    "            for i in range(start,end+1):\n",
    "                search = \"(\"+str(i)+\",\"\n",
    "                try:\n",
    "                    start_index = insert.index(search)\n",
    "                except:\n",
    "                    continue\n",
    "                count_quotes = 0\n",
    "                #title\n",
    "                third = None\n",
    "                fourth = None\n",
    "                #author\n",
    "                fifth = None\n",
    "                sixth = None\n",
    "                #series\n",
    "                seventh = None\n",
    "                eighth = None\n",
    "                #language\n",
    "                eleventh = None\n",
    "                twelfth = None\n",
    "\n",
    "                for j in range(start_index, len(insert)):\n",
    "                    if insert[j] == \"'\":\n",
    "                        count_quotes += 1\n",
    "                        if count_quotes == 3:\n",
    "                            third = j\n",
    "                        elif count_quotes == 4:\n",
    "                            fourth = j   \n",
    "                        elif count_quotes == 5:\n",
    "                            fifth = j\n",
    "                        elif count_quotes == 6:\n",
    "                            sixth = j\n",
    "                        elif count_quotes==7:\n",
    "                            seventh=j\n",
    "                        elif count_quotes==8:\n",
    "                            eighth=j\n",
    "                        elif count_quotes==11:\n",
    "                            eleventh=j\n",
    "                        elif count_quotes==12:\n",
    "                            twelfth=j\n",
    "                            break\n",
    "                if third is not None and fourth is not None and fifth is not None and sixth is not None and\\\n",
    "                     seventh is not None and eighth is not None and eleventh is not None and twelfth is not None:\n",
    "                    file2.write(str(i) + \",\" + insert[third+1:fourth].replace(\",\",\"\")+\",\"+insert[fifth+1:sixth].replace(\",\",\"\") + \",\"+insert[seventh+1:eighth].replace(\",\",\"\") + \",\"+insert[eleventh+1:twelfth].replace(\",\",\"\") + \"\\n\")\n",
    "            \n",
    "            return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......28667\n",
      ".........64594\n",
      ".........99734\n",
      ".........131011\n",
      ".........168152\n",
      ".........202876\n",
      ".........237539\n",
      ".........272392\n",
      ".........306894\n",
      ".........341170\n",
      ".........375613\n",
      ".........407835\n",
      ".........435038\n",
      ".........463659\n",
      ".........491762\n",
      ".........522133\n",
      ".........552519\n",
      ".........582332\n",
      ".........611113\n",
      ".........639968\n",
      ".........670039\n",
      ".........700457\n",
      ".........730285\n",
      ".........760153\n",
      ".........787376\n",
      ".........813762\n",
      ".........843766\n",
      ".........875480\n",
      ".........905937\n",
      ".........933558\n",
      ".........961511\n",
      ".........990420\n",
      ".........1017566\n",
      ".........1044951\n",
      ".........1073651\n",
      ".........1103071\n",
      ".........1132267\n",
      ".........1160012\n",
      ".........1187394\n",
      ".........1215199\n",
      ".........1242655\n",
      ".........1270351\n",
      ".........1298656\n",
      ".........1327314\n",
      ".........1356326\n",
      ".........1384880\n",
      ".........1413619\n",
      ".........1442123\n",
      ".........1470731\n",
      ".........1498009\n",
      ".........1526232\n",
      ".........1560137\n",
      ".........1594475\n",
      ".........1628151\n",
      ".........1654810\n",
      ".........1681302\n",
      ".........1708012\n",
      ".........1734676\n",
      ".........1761319\n",
      ".........1787822\n",
      ".........1814451\n",
      ".........1841020\n",
      ".........1869401\n",
      ".........1901824\n",
      ".........1928053\n",
      ".........1954079\n",
      ".........1979769\n",
      ".........2005086\n",
      ".........2031945\n",
      ".........2063127\n",
      ".........2091752\n",
      ".........2118612\n",
      ".........2146131\n",
      ".........2173047\n",
      ".........2206484\n",
      ".........2237857\n",
      ".........2270355\n",
      ".........2302829\n",
      ".........2334571\n",
      ".........2366872\n",
      ".........2400277\n",
      ".........2433859\n",
      ".........2463848\n",
      ".........2493738\n",
      ".........2523093\n",
      ".........2550627\n",
      ".........2580116\n",
      ".........2609689\n",
      ".........2639958\n",
      ".........2670155\n",
      ".........2700034\n",
      ".........2729918\n",
      ".........2761370\n",
      ".........2790952\n",
      ".........2821789\n",
      "......"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     end \u001b[38;5;241m=\u001b[39m sql_to_csv_NEW(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatements_fiction/statement\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsvs_fiction/csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,end\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(end,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36msql_to_csv_NEW\u001b[0;34m(statement_file, out_file, start)\u001b[0m\n\u001b[1;32m      6\u001b[0m insert \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Searching from \",start,\" to +3000\")\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m insert\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(start)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m found_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m last_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Finished filling up till file 67 (ID 114675) so need to fill after that. There was an issue.\n",
    "end=0\n",
    "for i in range(3,1000):\n",
    "    end = sql_to_csv_NEW(\"statements_fiction/statement\"+ str(i) + \".txt\",\"csvs_fiction/csv\"+ str(i) + \".txt\",end+1)\n",
    "    if(i%10==0):\n",
    "        print(end,end='\\n')\n",
    "    else:\n",
    "        print(\".\",end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that this is around the point where my assumptions go false again. Because all the IDs are now not in order but are randomly jumbled up, so I'll have to search the entire insert string to find each occurrence of an ID to find the largest ID. Might not be worth it, huh. \n",
    "\n",
    "But also btw the algorithm is inefficient - because I don't need to incrementally find each ID. To get the maximum ID, all I need to do is check the last ID on the list. Start from the back, and the first regex matching \"(N,\" where N is some integer - find that N and take that as the largest number. Then search for all integers in the range you've got and if you don't find a particular index  ignore that."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1e3bcc722a39238b6a7f93d00740f2997fe4fe04527d6796cad45df3c0deb"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit ('mne': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}